{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5bf3ecb",
   "metadata": {},
   "source": [
    "# 监督学习与非监督学习笔记\n",
    "\n",
    "## 1. 监督学习\n",
    "\n",
    "### 定义\n",
    "监督学习使用**有标签数据**训练模型。每个样本包含特征 $x$ 和标签 $y$，模型学习从 $x$ 到 $y$ 的映射。\n",
    "\n",
    "- **目标**：找到函数 $h(x)$，对新输入 $x$ 预测 $y$。\n",
    "\n",
    "### 类型\n",
    "1. **回归**：\n",
    "   - 预测连续值。\n",
    "   - 例子：预测房价。\n",
    "   - 算法：线性回归。\n",
    "2. **分类**：\n",
    "   - 预测离散类别（如 0/1）。\n",
    "   - 例子：垃圾邮件检测。\n",
    "   - 算法：逻辑回归、SVM。\n",
    "\n",
    "### 工作原理\n",
    "1. **训练**：\n",
    "   - 数据集：$\\{(x_1, y_1), (x_2, y_2), \\dots, (x_m, y_m)\\}$.\n",
    "   - 最小化代价函数（如均方误差），调整参数。\n",
    "   - 优化：梯度下降。\n",
    "2. **预测**：\n",
    "   - 对新 $x$，输出 $h(x)$。\n",
    "3. **特征缩放**：\n",
    "   - 标准化：$x' = \\frac{x - \\mu}{\\sigma}$，加速梯度下降。\n",
    "\n",
    "### 例子\n",
    "- **回归**：用面积预测房价。\n",
    "  - 数据：面积 $x = 100$ 平米，房价 $y = 500$ 万。\n",
    "  - 模型：$h(x) = \\theta_0 + \\theta_1 x$.\n",
    "- **分类**：用学习时间、成绩预测考试及格。\n",
    "  - 数据：时间 5 小时，成绩 80 分，$y = 1$（及格）。\n",
    "  - 模型：逻辑回归，$h(x) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2)}}$.\n",
    "\n",
    "### 常见算法\n",
    "- 线性回归\n",
    "- 逻辑回归\n",
    "- 决策树\n",
    "- 支持向量机 (SVM)\n",
    "- 神经网络\n",
    "\n",
    "### 应用\n",
    "- 房价预测\n",
    "- 垃圾邮件分类\n",
    "- 疾病诊断\n",
    "\n",
    "## 2. 非监督学习\n",
    "\n",
    "### 定义\n",
    "非监督学习处理**无标签数据**，只有特征 $x$，无 $y$。模型发现数据中的模式。\n",
    "\n",
    "- **目标**：提取规律，如分组或降维。\n",
    "\n",
    "### 类型\n",
    "1. **聚类**：\n",
    "   - 将数据分组，簇内样本相似。\n",
    "   - 例子：客户分群。\n",
    "   - 算法：K 均值聚类。\n",
    "2. **降维**：\n",
    "   - 压缩高维数据，保留主要信息。\n",
    "   - 例子：图像压缩。\n",
    "   - 算法：主成分分析 (PCA)。\n",
    "\n",
    "### 工作原理\n",
    "1. **聚类**：\n",
    "   - 自动分组，簇内相似，簇间不同。\n",
    "   - 例：K-Means 随机初始化簇中心，迭代更新。\n",
    "2. **降维**：\n",
    "   - 投影到低维空间。\n",
    "   - 例：PCA 将高维数据降到 2 维。\n",
    "\n",
    "### 例子\n",
    "- **聚类**：根据购买记录分群。\n",
    "  - 数据：客户 A (高金额、低频率)，客户 B (低金额、高频率)。\n",
    "  - 结果：分为“高端客户”“频繁小额客户”。\n",
    "- **降维**：压缩图像。\n",
    "  - 数据：100x100 像素 (10000 维)。\n",
    "  - 结果：PCA 降到 2 维。\n",
    "\n",
    "### 常见算法\n",
    "- K 均值聚类\n",
    "- 层次聚类\n",
    "- 主成分分析 (PCA)\n",
    "- 自编码器\n",
    "\n",
    "### 应用\n",
    "- 客户细分\n",
    "- 图像压缩\n",
    "- 异常检测\n",
    "\n",
    "## 3. 监督学习 vs 非监督学习\n",
    "\n",
    "| 特性       | 监督学习                 | 非监督学习             |\n",
    "|------------|-------------------------|-----------------------|\n",
    "| 数据       | 有标签 (特征+标签)      | 无标签 (只有特征)     |\n",
    "| 目标       | 预测标签               | 发现模式             |\n",
    "| 例子       | 房价预测、垃圾邮件分类  | 客户分群、数据降维   |\n",
    "| 算法       | 线性回归、逻辑回归      | K-Means、PCA         |\n",
    "| 挑战       | 需标注数据，成本高      | 结果需人工解释       |\n",
    "\n",
    "## 4. 吴恩达课程重点\n",
    "- **监督学习** (Week 1-3)：\n",
    "  - 线性回归：最小化均方误差。\n",
    "  - 逻辑回归：预测概率。\n",
    "  - 特征缩放：加速收敛。\n",
    "- **非监督学习** (Week 8-9)：\n",
    "  - K-Means：分组数据。\n",
    "  - PCA：降维。\n",
    "- **建议**：用 Python 重现作业，可视化结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df728eba",
   "metadata": {},
   "source": [
    "# 线性回归笔记\n",
    "\n",
    "## 1. 线性回归概述\n",
    "\n",
    "### 定义\n",
    "线性回归是一种**监督学习**算法，用于预测**连续值**输出。基于输入特征（如房屋面积），预测一个数值（如房价）。\n",
    "\n",
    "- **核心目标**：找到一个线性函数 $h(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n$，使预测值接近真实值。\n",
    "- $x_1, x_2, \\dots, x_n$：特征（如面积、房间数）。\n",
    "- $\\theta_0, \\theta_1, \\dots, \\theta_n$：模型参数（通过训练学习）。\n",
    "- $h(x)$：预测值。\n",
    "\n",
    "### 例子\n",
    "- **问题**：用房屋面积预测房价。\n",
    "- **数据**：面积 $x = 100$ 平米，房价 $y = 500$ 万。\n",
    "- **模型**：$h(x) = \\theta_0 + \\theta_1 x$，目标是学到合适的 $\\theta_0, \\theta_1$，如 $h(x) = 50 + 4.5x$。\n",
    "\n",
    "## 2. 损失函数\n",
    "\n",
    "### 定义\n",
    "损失函数（Cost Function）衡量模型预测的准确性。线性回归通常使用**均方误差（MSE）**：\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "- $m$：样本数量。\n",
    "- $h(x^{(i)})$：第 $i$ 个样本的预测值。\n",
    "- $y^{(i)}$：第 $i$ 个样本的真实值。\n",
    "- **目标**：调整 $\\theta$ 参数，最小化 $J(\\theta)$。\n",
    "\n",
    "### 直观理解\n",
    "- 损失函数计算预测值和真实值误差的平方平均值。\n",
    "- 例：预测房价 500 万，实际 600 万，误差 $(500 - 600)^2 = 10000$。\n",
    "- 除以 $2m$ 是为了求导方便。\n",
    "\n",
    "## 3. 梯度下降\n",
    "\n",
    "### 定义\n",
    "梯度下降（Gradient Descent）是一种优化算法，通过迭代更新参数 $\\theta$，找到损失函数 $J(\\theta)$ 的最小值。\n",
    "\n",
    "### 公式\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n",
    "$$\n",
    "- $\\theta_j$：第 $j$ 个参数（如 $\\theta_0, \\theta_1$）。\n",
    "- $\\alpha$：学习率（Learning Rate），控制步长（如 0.01）。\n",
    "- $\\frac{\\partial J}{\\partial \\theta_j}$：损失函数对 $\\theta_j$ 的偏导数（梯度）。\n",
    "\n",
    "### 单变量线性回归的梯度\n",
    "对于 $h(x) = \\theta_0 + \\theta_1 x$：\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)})\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^m (h(x^{(i)}) - y^{(i)}) x^{(i)}\n",
    "$$\n",
    "\n",
    "### 直观理解\n",
    "- 损失函数像一个碗，$\\theta$ 是碗里的点，梯度下降每次沿最陡方向走一小步。\n",
    "- **学习率 $\\alpha$**：\n",
    "  - 太小：收敛慢，像乌龟爬。\n",
    "  - 太大：可能跳过最低点，甚至发散。\n",
    "- 例：初始 $\\theta_0 = 0, \\theta_1 = 0$，通过迭代更新，可能学到 $\\theta_0 \\approx 50, \\theta_1 \\approx 4.5$。\n",
    "\n",
    "## 4. 特征缩放\n",
    "\n",
    "### 定义\n",
    "特征缩放（Feature Scaling）将特征值调整到相似范围（如 0 到 1 或均值 0、方差 1），加速梯度下降收敛。\n",
    "\n",
    "### 方法\n",
    "1. **标准化（Standardization）**：\n",
    "   $$\n",
    "   x' = \\frac{x - \\mu}{\\sigma}\n",
    "   $$\n",
    "   - $\\mu$：特征均值，$\\sigma$：标准差。\n",
    "   - 结果：特征均值为 0，方差为 1。\n",
    " - 标准化原理是什么？\n",
    " - 为什么÷标准差而不是÷均值？\n",
    "   如果÷均值（错误）：\n",
    "   只能看出\"相对平均线的位置\"\n",
    "\n",
    "   但看不出\"这个差距算不算大\"\n",
    "\n",
    "   5.0 vs 5.5：在专业比赛中是巨大差距，在业余比赛中是小差距\n",
    "\n",
    "   如果÷标准差（正确）：\n",
    "   能看出\"这个差距在当前的竞争环境中算多大\"\n",
    "\n",
    "   考虑了数据的分散程度\n",
    "\n",
    "   使得比较更加公平合理\n",
    "2. **归一化（Normalization）**：\n",
    "   $$\n",
    "   x' = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n",
    "   $$\n",
    "   - 结果：特征缩放到 $[0, 1]$。\n",
    "   - 注意：当有异常数据时，比如最大值非常的大的话就会产生较大的影响\n",
    "\n",
    "### 为什么需要？\n",
    "- 特征量纲差异大（如面积 100 平米，房间数 3 个），会导致梯度下降路径扭曲，收敛慢。\n",
    "- 缩放后，损失函数更“圆”，梯度下降更快找到最低点。\n",
    "- \n",
    "- 注意这里的量纲是指数据的单位，比如我们在去建立一个函数y=x的平方\n",
    "- 这里y指带一个房子的价格（单位为万），x指代房子的面积（单位为平方米）\n",
    "- 这里我们是通过给两个变量规定了一个对双方都有好处的单位\n",
    "- 如x=1平方米时，y=1万元，而不是10000元\n",
    "- 所以特征缩放的本质就是通过规定单位来实现对函数的曲折程度的改变，从而去影响函数切线斜率大小\n",
    "\n",
    "### 例子\n",
    "- 数据：面积 $x_1 = 100$ 平米，房间数 $x_2 = 3$。\n",
    "- 未缩放：面积主导梯度更新，收敛慢。\n",
    "- 标准化后：$x_1' \\approx 0, x_2' \\approx 0$，梯度下降更平稳。\n",
    "\n",
    "## 5. 工作流程\n",
    "1. 准备数据：收集特征 $X$ 和标签 $y$，进行特征缩放。\n",
    "2. 定义模型：$h(x) = \\theta_0 + \\theta_1 x_1 + \\dots$.\n",
    "3. 定义损失函数：计算 $J(\\theta)$.\n",
    "4. 梯度下降：迭代更新 $\\theta$，最小化 $J(\\theta)$.\n",
    "5. 预测：用训练好的 $\\theta$ 对新数据预测。\n",
    "\n",
    "## 6. 例子：房价预测\n",
    "- **数据**：面积 $x = [100, 150, 200]$ 平米，房价 $y = [500, 700, 900]$ 万。\n",
    "- **特征缩放**：将面积标准化，均值 $\\approx 150$，标准差 $\\approx 50$。\n",
    "- **模型**：$h(x) = \\theta_0 + \\theta_1 x_{\\text{scaled}}$。\n",
    "- **训练**：用梯度下降，初始 $\\theta_0 = 0, \\theta_1 = 0$，学习率 $\\alpha = 0.01$，迭代 1000 次。\n",
    "- **结果**：可能学到 $\\theta_0 \\approx 700, \\theta_1 \\approx 4$，预测面积 150 平米房价 $\\approx 700$ 万。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
